# -*- coding: utf-8 -*-
"""llama-chat-finetune_hryu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RK3dBpa2EqlnOAx9MipAGxLbiPYZNaKy

#  **AIFrenz GPU bootcamp**
##   End-to-End LLM finetune

by Hyungon Ryu | NVIDIA Sr. Solution Architect

ë³¸ ì»¨í…ì¸ ëŠ” NVIDIA OpenHackathon íŒ€ì´ ìš´ì˜í•˜ëŠ” GPU Bootcamp contentsë¥¼ Google Colabì— ë§ê²Œ ë³€ê²½í•˜ì˜€ê¸° ë•Œë¬¸ì—, ëª‡ëª‡ ìˆ˜ë™ ì„¤ì • ë° ì½”ë“œ ë³€ê²½í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë“¤ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ì˜¤ë¦¬ì§€ë„ ì»¨í…ì¸ ëŠ” ë‹¤ìŒì˜ ë§í¬ë¥¼ í†µí•´ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. [End-to-End-LLM](https://github.com/openhackathons-org/End-to-End-LLM)

<image src="https://raw.githubusercontent.com/openhackathons-org/End-to-End-LLM/refs/heads/main/workspace-llm-use-case/jupyter_notebook/llm-use-case/images/llama2-chat-arc.png">

# ì‹¤ìŠµí™˜ê²½ ì†Œê°œ
- ê°œë°œí™˜ê²½ :  Google Colab T4 ì´ìƒ GPU ( 15GiB)
- Toolkit : CUDA/cuDNN, pytorch, transformer, trl, peft, bitsandbytes, datasets
- Model : Meta llama2 7B-chat
- Dataset : Guanaco, Meerkat

### GPU í™˜ê²½ í™•ì¸
`$ !nvidia-smi` ëª…ë ¹ì„ í†µí•´ í˜„ì¬ ì„¤ì¹˜ë˜ì–´ ìˆëŠ” GPUë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì•„ë˜ì™€ ê°™ì´ GPU ì¢…ë¥˜, ë©”ëª¨ë¦¬ í¬ê¸°, ì„¤ì¹˜ëœ ë“œë¼ì´ë²„ ì •ë³´ ë“±

```
-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |
| N/A   47C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
```
"""

!nvidia-smi

"""## ì‹¤ìŠµì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
LLM finetuneì„ ìœ„í•´ì„œ hugginface transformer, datasets, peft, trlì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.
quantizationì„ ìœ„í•´ì„œ bitsandbytesë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
ëª¨ë¸ íŒŒë¼ë¯¸í„°, ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê¸° ìœ„í•´ì„œ google gdownì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.
ì„¤ì¹˜ëŠ” ë„¤íŠ¸ì› í™˜ê²½ì— ë”°ë¼ ì•½ 5ë¶„ ì •ë„ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤.

colabí™˜ê²½ì—ì„œ NEMO toolkit ì„¤ì¹˜ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ìƒëµí•˜ì˜€ìŠµë‹ˆë‹¤.
"""

# Commented out IPython magic to ensure Python compatibility.
#  %%time
!pip install --upgrade pip
!pip install transformers datasets langdetect peft trl bitsandbytes gdown

"""## ì‹¤ìŠµì— í•„ìš”í•œ utility íŒŒì¼ë“¤ì„ ë¼ìš´ë¡œë“œí•©ë‹ˆë‹¤.
ëª¨ë‘ OpenHackathoníŒ€ì´ ì œì‘í•œ GPU Bootcamp ì»¨í…ì¸ ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.  
"""

# download python script from OpenHackathon github
import gdown
import os

os.makedirs("source", exist_ok=True)
scripts = {
    "/content/source/download-guanco-ds.py": "https://github.com/openhackathons-org/End-to-End-LLM/raw/refs/heads/main/workspace-llm-use-case/source_code/Llama2/download-guanco-ds.py",
    "/content/source/download-llama2-chat.py": "https://github.com/openhackathons-org/End-to-End-LLM/raw/refs/heads/main/workspace-llm-use-case/source_code/Llama2/download-llama2-chat.py",
    "/content/source/download-llama2.py":"https://github.com/openhackathons-org/End-to-End-LLM/raw/refs/heads/main/workspace-llm-use-case/source_code/Llama2/download-llama2.py",
    "/content/source/convert_llama_weights_to_hf.py":"https://github.com/openhackathons-org/End-to-End-LLM/raw/refs/heads/main/workspace-llm-use-case/source_code/Llama2/llama/convert_llama_weights_to_hf.py",
    "/content/source/configuration_llama.py":"https://github.com/openhackathons-org/End-to-End-LLM/raw/refs/heads/main/workspace-llm-use-case/source_code/Llama2/llama/configuration_llama.py",
    "/content/source/modeling_flax_llama.py":"https://github.com/openhackathons-org/End-to-End-LLM/raw/refs/heads/main/workspace-llm-use-case/source_code/Llama2/llama/modeling_flax_llama.py",
    "/content/source/modeling_llama.py":"https://github.com/openhackathons-org/End-to-End-LLM/raw/refs/heads/main/workspace-llm-use-case/source_code/Llama2/llama/modeling_llama.py",
    "/content/source/tokenization_llama.py":"https://github.com/openhackathons-org/End-to-End-LLM/raw/refs/heads/main/workspace-llm-use-case/source_code/Llama2/llama/tokenization_llama.py",
    "/content/source/tokenization_llama_fast.py":"https://github.com/openhackathons-org/End-to-End-LLM/raw/refs/heads/main/workspace-llm-use-case/source_code/Llama2/llama/tokenization_llama_fast.py",
    "/content/source/download-meerkat-ds.py":"https://github.com/yhgon/bootcamp_kr/raw/refs/heads/main/2025/0703_AIFrenz/download-meerkat-ds.py",


}

for fname, url in scripts.items():
    print(f"Downloading {fname} â€¦")
    gdown.download(url, fname, quiet=False)

"""## ì‹¤ìŠµ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ì‹¤ìŠµì— í•„ìš”í•œ Llama-2 checkpointë¥¼ ë‹¤ìš´ë¡œë“œ ë°›ìŠµë‹ˆë‹¤.
Huggingface ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ ë°›ëŠ” ê²ƒë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.
*ì£¼ì˜*
ìš©ëŸ‰ì´ í° 27GB íŒŒì¼ì„ ë°›ê¸° ë•Œë¬¸ì— ì•½ 3~10ë¶„ ì´ìƒì˜ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤.

ë‹¤ìš´ë¡œë“œ í›„ **`ëŸ°íƒ€ì„ ì—°ê²°ì„ í—¤ì œ`**ë¥¼ ìˆ˜í–‰í•˜ë©´  Colabì´ ì´ˆê¸°í™”ë˜ì–´ íŒŒì¼ì„ ì²˜ìŒë¶€í„°  ë‹¤ì‹œ ë°›ìœ¼ì…”ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
Google Driveë¥¼ ì—°ë™í•˜ëŠ” ê²½ìš° ë³µì‚¬í•´ ë†“ìœ¼ì‹œë©´ ë°˜ë³µ ë‹¤ìš´ë¡œë“œë¥¼ í”¼í•˜ê³  ë‹¤ìš´ë¡œë“œí•œ ì´ë¯¸ì§€ë¥¼ ê³„ì† ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Download Llama-2 checkpoints via gdown
# import gdown
# import os
# 
# os.makedirs("model", exist_ok=True)
# links = {
#     "/content/model/Llama-2-7b-chat.tar": "https://drive.google.com/uc?id=1rfMCxT8fUQwQ5Pe6z7kDgK3nAZ6FyEeT&confirm=t",
# }
# 
# for out, url in links.items():
#     print(f"Downloading {out} â€¦")
#     gdown.download(url, out, quiet=False)
#

"""í•˜ë“œë””ìŠ¤í¬ë¥¼ ì ˆì•½í•˜ê¸° ìœ„í•´ì„œ ì••ì¶•ì„ í‘¼ í›„ ì••ì¶• íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.
ì•½ 2ë¶„ ì •ë„ì˜ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤.
"""

# Commented out IPython magic to ensure Python compatibility.
#  %%time
!tar -xf model/Llama-2-7b-chat.tar -C model && rm model/Llama-2-7b-chat.tar

"""## Inference í…ŒìŠ¤íŠ¸

### *ì£¼ì˜*
Hugginface transformer pipelineì„ ì´ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìš´ë¡œë“œí•œ ì²´í¬í¬ì¸íŠ¸ë¥¼
Hugginface Transformer ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë§ê²Œ ë³€í™˜ì„ í•´ì¤˜ì•¼í•©ë‹ˆë‹¤.
ë³€í™˜ ë° ì €ì¥ì— ì•½ 3ë¶„ ì •ë„ì˜ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤.

### Convert model to Hugging Face Transformers Format
We will convert our model checkpoint to the Hugging Face transformer format. The benefits are as follows:  

- Delivers a convenient way to finetune the Llama2 model with limited GPU computing resources (like laptops, workstations, or Google Colab) through the use of some technique that a transformer-compatible
- Ability to immediately use a model on a given input text using transformer Pipelines.
- The use of transformers `Pipelines` group together the pre-trained model with the preprocessing that was used during that training to enable quick inferencing.


To convert our model checkpoint, we use the `convert_llama_weights_to_hf.py` script located on [GitHub](https://github.com/cedrickchee/transformers-llama/tree/llama_push/src/transformers/models/llama).

- `--input_dir`: denotes the directory to the llama model to be converted
- `--model_size`: represents the llama model parameter size
- `--out_dir`: the directory to save the converted model

<img src="https://raw.githubusercontent.com/openhackathons-org/End-to-End-LLM/refs/heads/main/workspace-llm-use-case/jupyter_notebook/llm-use-case/images/llama-hf.png" height="550px" width="1200px" />

If you already have the Hugging Face format of the model `Llama-2-7b-chat-hf`, you can skip running the cell below.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# !python source/convert_llama_weights_to_hf.py \
#     --input_dir /content/model/Llama-2-7b-chat \
#     --model_size 7B \
#     --output_dir /content/model/Llama-2-7b-chat-hf

"""**Exepected Output:**
```python
...
https://github.com/huggingface/transformers/pull/24565
Fetching all parameters from the checkpoint at ../model/Llama-2-7b-chat.
Loading the checkpoint in a Llama model.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:06<00:00,  4.95it/s]
Saving in the Transformers format.

```

## chat prompt í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

HF pipelineì„ í†µí•´ì„œ ê°„ë‹¨í•œ promptë¥¼ ì‹¤í–‰í•´ë³´ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.
"""

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

BASE = "/content/model/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(BASE, trust_remote_code=True)
model     = AutoModelForCausalLM.from_pretrained(BASE)

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_length=100)
out = pipe("<s>[INST] What is astrophotography? [/INST]")[0]["generated_text"]
print(out)

"""## Out of Memory ì—ëŸ¬ê°€ ë°œìƒí•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

"""

!nvidia-smi

"""## 4-bit ì–‘ìí™”


Several ways and algorithms to quantize a model including can be found [here](https://huggingface.co/docs/peft/main/en/developer_guides/quantization). A library to easily implement quantization and integrate with transformers is the `bitsandbytes` library. The library provides config parameters to quantize a model to 8 or 4 bits using the `BitsAndBytesConfig` class. The 4 bits parameters used in the cell below are described as follows:

- **load_in_4bit**: set `True` to quantize the model to 4-bits when you load it
- **bnb_4bit_quant_type**: set to `"nf4"` to use a special 4-bit data type for weights initialized from a normal distribution
- **bnb_4bit_use_double_quant**: set `True` to use a nested quantization scheme to quantize the already quantized weights
- **bnb_4bit_compute_dtype**: set to `torch.float16` or `torch.bfloat16` to use bfloat16 for faster computation

Run the cell below to set the 4-bit quantization for our model.

## **ì£¼ì˜**
ë©”ëª¨ë¦¬ ì´ìŠˆë¡œ ì¸í•´ ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘ì„ í•˜ëŠ” ê²½ìš°  **`ì„¸ì…˜ ë‹¤ì‹œ ì‹œì‘`**ì„ ëˆ„ë¥´ì‹œë©´ ë©ë‹ˆë‹¤.

ì ˆëŒ€ë¡œ **`ëŸ°íƒ€ì„ ì—°ê²°í•´ì œ`**ë¥¼ ëˆ„ë¥´ë©´ ì•ˆë©ë‹ˆë‹¤. ì„¤ì¹˜ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë‹¤ìš´ë¡œë“œí•œ ì²´í¬í¬ì¸íŠ¸, ë°ì´í„°ê°€ ëª¨ë‘ ì‚¬ë¼ì§‘ë‹ˆë‹¤.
"""

!nvidia-smi

import os
import torch
import json
from datasets import load_dataset, load_from_disk
from langdetect import detect
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
import re

import warnings
warnings.filterwarnings("ignore")

# In some cases where you have access to limited computing resources, you might have to uncomment os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64" if you run into not enough memory issue
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"

"""15GB ë©”ëª¨ë¦¬ ë‚´ì—ì„œ inference /finetuneì„ ì§„í–‰í•˜ê¸° ìœ„í•´ì„œ
bitsandbytesë¥¼ í™œì„±í™”í•´ì•¼í•©ë‹ˆë‹¤.
"""

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False,
)

BASE = "/content/model/Llama-2-7b-chat-hf"

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model = AutoModelForCausalLM.from_pretrained(
#     BASE,
#     quantization_config=quant_config,
#     device_map={"": 0}
# )
# model.config.use_cache = False
# model.config.pretraining_tp = 1

"""ì‚¬ìš©ë©”ëª¨ë¦¬ë¥¼ í™•ì¸í•´ë³´ë©´ 6GBì •ë„ë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
Colabí™˜ê²½ì—ì„œëŠ” ì ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìœ„í•´ bitsandbytesì˜ ì–‘ìí™”ë¥¼ í™œì„±í™”í•´ì•¼ í•©ë‹ˆë‹¤.
"""

!nvidia-smi

"""### tokenizerë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""

tokenizer = AutoTokenizer.from_pretrained(BASE, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

"""### HF pipelineì„ ì •ì˜í•©ë‹ˆë‹¤."""

def run_inference(prompt,max_length=200 ):
    inf_pipeline = pipeline(model=model, tokenizer=tokenizer, max_length=max_length, task="text-generation")
    prompt = f"<s>[INST] {prompt} [/INST]"
    result = inf_pipeline(prompt)
    print(result[0]['generated_text'])

prompt = "explain what is astrophotography?"
run_inference(prompt)

!nvidia-smi

"""# Finetune

<img src="https://blogs.nvidia.co.kr/wp-content/uploads/sites/16/2023/04/Transformer-apps-672x459-1.jpg" width=800>

# Finetune llama2 model with custom dataset

## ë°ì´í„° ì „ì²˜ë¦¬ (Data Preprocessing)

We consider [openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) as our choice dataset for finetuning. The openassistant-guanaco dataset is a subset of the [Open Assistant Conversations](https://huggingface.co/datasets/OpenAssistant/oasst1/tree/main) dataset that contains only the highest-rated paths in the conversation tree, with a total of 9,85k training samples and 518 test samples. The OpenAssistant Conversations (OASST1) is a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers ([source](https://huggingface.co/datasets/OpenAssistant/oasst1)).

### Dataset Structure

Each row in the `openassistant-guanaco` jsonl dataset is a text dictionary that consists of `human` instructions and `Assistant` that provide context to draw the response to the instruction. Within the `text`, the `human` and `Assistant` fields are separated with three `###` delimiters that denote the start and end of positions.  

```bash

{ "text": "### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to...
           ### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term  is...   
           ### Human: Now explain it to a dog"
}
{ "text": "### Human: \u00bfCUales son las etapas del desarrollo y en qu\u00e9 consisten seg\u00fan Piaget?
           ### Assistant: Jean Piaget fue un psic\u00f3log suizo que propuso una teor\u00eda sobre el desarrollo cognitivo...
           ### Human: \u00bfHay otras teor\u00edas sobre las etapas del desarrollo que reafirmen o contradigan a la teor\u00eda de Piaget?"
}
{"text": "### Human: Can you give me an example of a python script that opens an api point and serves a string?
          ### Assistant: Sure! Here's an example Python script that uses the Flask web framework to create a simple API endpoint that serves a string:\n\n`         \nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello_world():\n    return 'Hello, world!'\n\nif __name__ ==  \n ...   
          ### Human: What changes would you need to make to the code above to serve a JSON object instead of a string?
          ### Assistant: To serve a JSON object instead of a string, you can modify the \"hello_world()\" function to return a JSON response using the  Flask \"jsonify\" function. Here's an example of how to modify the previous code to serve a JSON object:\n\n... "
}
...

```

Please run the cell below to check if the dataset exists in the data directory; otherwise, uncomment the nested cell below to download it.

## *ì£¼ì˜*
ì†ŒìŠ¤ì½”ë“œë¥¼ í™•ì¸í•˜ì—¬ ë°ì´í„° ì €ì¥ìœ„ì¹˜ë¥¼ Colabí™˜ê²½ì— ë§ê²Œ ë‹¤ìŒê³¼ ê°™ì´ ì¡°ì •í•´ì£¼ì„¸ìš”
`/content/source/download-guanco-ds.py`

ë³€ê²½ì „
```
output_eva = '../data/openassistant_best_replies_eval.jsonl'
output_train = '../data/openassistant_best_replies_train.jsonl'

```

ë³€ê²½í›„
```
output_eva = '/content/data/openassistant_best_replies_eval.jsonl'
output_train = '/content/data/openassistant_best_replies_train.jsonl'

```
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # download dataset.
# import os
# 
# os.makedirs("data", exist_ok=True)
# %time !python3 source/download-guanco-ds.py

!ls -alh data

"""**Expected Output:**
```python
data:
  openassistant_best_replies_eval.jsonl   
 openassistant_best_replies_train.jsonl
...

```

Import all required libraries
"""

import os
import torch
import json
from datasets import load_dataset, load_from_disk
from langdetect import detect
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
import re
from peft import LoraConfig, PeftModel
from trl import SFTTrainer

import warnings
warnings.filterwarnings("ignore")

# In some cases where you have access to limited computing resources, you might have to uncomment os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64" if you run into not enough memory issue
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"

"""### Data Extraction

Let's execute the `read_jsonl` function below to read the dataset
"""

# define utils
def read_jsonl(file_path):
    with open(file_path) as f:
        data = [json.loads(line) for line in f]
        return data

"""Let's perform the following steps in the cell below:
- Set the path to the train and test jsonl files
- Read both files using the `read_jsonl` function
- Extract 5k samples from the training set
- Display the samples to see the content and format
"""

#set path to jsonl files
train_path = '/content/data/openassistant_best_replies_train.jsonl'
test_path = '/content/data/openassistant_best_replies_eval.jsonl'

# read the files
raw_train_data = read_jsonl(train_path)
raw_test_data = read_jsonl(test_path)

# extract 5000 samples
train_samples = raw_train_data[:5000]

print("length of traning samples: ", len(train_samples))
train_samples[:10]

"""# ë°ì´í„° í¬ë§·
guanaco ë°ì´í„°ëŠ” jsonl í¬ë§·ìœ¼ë¡œ í•˜ë‚˜í•˜ë‚˜ì˜ ëŒ€í™”ê°€ ***`'text'`*** ë¡œ ì…ë ¥ë˜ì–´ ìˆê³  íŠ¹íˆ ëŒ€í™” ì§€ë¬¸ì„ í™”ìì™€ AIë¥¼ `### Human:`** ê³¼ **`### Assistant:`** ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬  multi turn ëŒ€í™” ë°ì´í„°ì…‹ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ìµœê·¼ ì‚¬ìš©í•˜ëŠ” í•™ìŠµ ë°ì´í„°ì…‹ì€ System Role ë“±ì´ í¬í•¨ëœ instruction ë°ì´í„°ì…‹,  OpenAI Message í¬ë§· ë“±ì´ ë§ì´ ì‚¬ìš©ë˜ê³  ìˆê¸° ë•Œë¬¸ì—
ì‹¤ì œ raw dataë¥¼ ì‚´í´ë´ì•¼ í•©ë‹ˆë‹¤.  ë˜í•œ, ê° ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” í•™ìŠµë°ì´í„° í¬ë§·ì„ ì²´í¬í•´ì•¼ í•©ë‹ˆë‹¤. ì‹¤ìŠµí•  ëª¨ë¸ì€ ê¸° í•™ìŠµëœ Meta llama2-chat ëª¨ë¸ì´ë¯€ë¡œ í¬ë§·ì„ ë§ì¶°ì£¼ëŠ” ì‘ì—…ì„ ì§„í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

**Expected output:**
```python

length of traning samples:  5000
[{'text': '### Human: Can you write a short introduction about the relevance of the term "monopsony" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: "Monopsony" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as ..'},
 ...
 {'text': '### Human: Â¿CUales son las etapas del desarrollo y en quÃ© consisten segÃºn Piaget?### Assistant: Jean Piaget fue un psicÃ³logo suizo que propuso una teorÃ­a sobre el desarrollo cognitivo humano que consta de cuatro etapas:\n\nEtapa sensoriomotora (0-2 aÃ±os): Durante esta etapa, el niÃ±o aprende a travÃ©s de sus sentidos y movimientos. Descubre que sus acciones pueden tener un impacto en el entorno y comienza a formarse una idea bÃ¡sica de objetividad y continuidad.\n\nEtapa preoperatoria (2-7 aÃ±os): En esta etapa, el niÃ±o comienza a desarrollar un pensamiento simbÃ³lico y a comprender que las cosas pueden representar a otras cosas. TambiÃ©n comienzan a desarrollar un pensamiento lÃ³gico y a comprender conceptos como la causa y el efecto.\n\nEtapa de ...?'},
 ...
 {'text': '### Human: Schreibe einen kurze und prÃ¤zise Konstruktionsbeschreibung zu einem Dreieck ABC mit c=6\xa0cm, h_c=5\xa0cm und Î³=40Â°. Î³ ist hierbei der von Seite c gegenÃ¼berliegende Winkel.### Assistant: Dreieck ABC ist ein rechtwinkliges Dreieck mit der SeitenlÃ¤nge c=6 cm als Hypotenuse. Die HÃ¶he h_c von c betrÃ¤gt 5 cm und der Winkel Î³ von c gegenÃ¼berliegend betrÃ¤gt 40Â°.### Human: Vielen Dank, das hat mir sehr weitergeholfen.'},
 {'text': '### Human: ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¸Ğ³Ñ€Ğµ Hytale### Assistant: Hytale - ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ¾ÑÑ‰Ğ°Ñ Ğ¸Ğ³Ñ€Ğ°-Ğ¿ĞµÑĞ¾Ñ‡Ğ½Ğ¸Ñ†Ğ°, Ğ¿Ñ€Ğ¸Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½Ğ°Ñ Hypixel Studios. ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ² ÑĞµĞ±Ğµ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ğ¸Ğ³Ñ€Ñ‹-Ğ¿ĞµÑĞ¾Ñ‡Ğ½Ğ¸Ñ†Ñ‹ Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ Ñ€Ğ¾Ğ»ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ³Ñ€Ñ‹, Hytale Ğ¿Ğ¾Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ¸Ğ³Ñ€Ğ¾ĞºĞ¾Ğ² Ğ² Ğ¶Ğ¸Ğ²Ğ¾Ğ¹ ĞºĞ¾ÑĞ¼Ğ¾Ñ, Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ÑÑĞºĞ°Ğ¿Ğ°Ğ´ Ğ¸ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑÑ‚Ğ²Ğ°.\n\nĞ’ Hytale Ğ³ĞµĞ¹Ğ¼ĞµÑ€Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¼Ñƒ, Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¸Ñ€Ñƒ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ğµ Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…. ĞÑ‚ Ğ²Ğ¾Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ· Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ´Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚ Ğ¿Ñ€Ğ¸ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¸Ğ»Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¸Ğ³Ñ€Ğ¾ĞºĞ°Ğ¼Ğ¸ - Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Hytale Ğ±ĞµĞ·Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹. Ğ˜Ğ³Ñ€Ğ¾ĞºĞ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ ..'},
 {'text': '### Human: ç§ã¯çŒ«ã¡ã‚ƒã‚“ãŒå¥½ããªã‚“ã§ã™ã‘ã©\nè²´æ–¹ã‚‚çŒ«ã¡ã‚ƒã‚“ãŒå¥½ãã§ã™ã‹?### Assistant: çŒ«ã¡ã‚ƒã‚“ãŒå¥½ããªã‚“ã§ã™ã­ã€‚\nç§ã‚‚çŒ«ã¡ã‚ƒã‚“ãŒå¥½ãã§ã™ã‚ˆï¼\nçŒ«ã¡ã‚ƒã‚“ã®ã©ã‚“ãªã¨ã“ã‚ãŒå¥½ãã§ã™ã‹ï¼Ÿ### Human: çŒ«ã¡ã‚ƒã‚“ã®å…¨ã¦ãŒå¥½ãã§ã™ï¼çŒ«ã¡ã‚ƒã‚“ã¯å¤©ä½¿ã§ã™ï¼ï¼### Assistant: ã‚ã‹ã‚Šã¾ã™ã‚ˆã€ãã®æ°—æŒã¡ï¼\nçŒ«ã¡ã‚ƒã‚“ã®æ„›ãã‚‹ã—ã„å§¿ã¯ã€å¤©ä½¿ã¿ãŸã„ã«å¯æ„›ã„ã§ã™ã‚ˆã­ï¼\nã‚ãªãŸã®ãŠå®¶ã«ã€çŒ«ã¡ã‚ƒã‚“ã¯ã„ã‚‹ã‚“ã§ã™ã‹ï¼Ÿ### Human: å‹¿è«–ã§ã™ï¼ã¨ã£ã¦ã‚‚å¯æ„›ãã¦ã€æ¯æ—¥ãŒå¹¸ã›ã§ã™ï¼'},

 {'text': "### Human: Quins sinÃ²nims amb altres dialectes d'aquesta llengua tÃ© nen o nena?### Assistant: AlÂ·lot o alÂ·lota, vailet o vaileta, manyac o manyaga, nin o nina, xiquet o xiqueta, xic o xica, marrec, minyÃ³ o minyona."},
...
```
"""

print( len(raw_train_data), len(train_samples) )

"""We can see from the content displayed that the file contains text written in different languages. For demonstration purposes, we want to consider only English text for training and prompting. Therefore, the train and test sets will be processed to filter out non-English text samples using a `detect` feature from the `spacy-langdetect` library. The English-only texts (`1778 training samples`) are saved as the new training set. The same process is applied to the test samples. Run the three two cells below to execute the filter process.     """

# filter non-Emglish word
def remove_nonEnglish_rows(ds):
    new_ds = []
    for row in (ds):
        if detect(str(row)) == 'en':
            new_ds.append(row)
    return new_ds

# save English text samples
os.makedirs("/content/data/filtered", exist_ok=True)

def save_jsonl(ds,filename):
    with open(f"/content/data/filtered/{filename}.jsonl", "w") as write_file:
            json.dump(ds, write_file, indent=4)
            print("dataset saved in jsonl format ....")

#calling filter function
filter_train_samples = remove_nonEnglish_rows(train_samples)
filter_test_samples = remove_nonEnglish_rows(raw_test_data )

print("len of training samples: ",len(filter_train_samples))
filter_train_samples[:10]

print( len(raw_train_data),  len(train_samples), len(filter_train_samples), )
print( len(raw_test_data),   len(filter_test_samples) )

"""**Expected output:**
```python
len of training samples:  1775
[{'text': '### Human: Can you write a short introduction about the relevance of the term "monopsony" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: "Monopsony" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as ...'},
 {'text': '### Human: Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?### Assistant: Sure! Let\'s say you want to build a model which can distinguish between images of cats and dogs. You gather your dataset, consisting of many cat and dog pictures. Then you put them through a neural net of your choice, which produces some representation for each image, a sequence of numbers like [0.123, 0.045, 0.334, ...]. The problem is, if your model is unfamiliar with cat and dog images, these representations will be quite random. At one time a cat and ...'},
 {'text': "### Human: I want to start doing astrophotography as a hobby, any suggestions what could i do?### Assistant: Getting started in astrophotography can seem daunting, but with some patience and practice, you can become a master of the craft. To begin, you'll need a good camera and lens, a tripod, and a dark sky location free of light pollution. You will also need to learn about the basics of astrophotography, such as what camera settings to use, how to capture star trails, and the best techniques for tracking celestial objects. You can also purchase or rent different types of telescopes, depending on what types of objects you want to capture. Additionally, it's important to keep up with the latest astrophotography news and trends. Once you have the necessary ..."},
...
```
"""

# set file names
save_train_filename = 'train'
save_test_filename = 'test'

# save file
save_jsonl(filter_train_samples, save_train_filename)
save_jsonl(filter_test_samples, save_test_filename)

"""### Data Transformation

In this section, we want to format our dataset to the Llama2 acceptable template for finetuning:

```python

<s>[INST] {human text} [/INST] {assistant/context} </s>
<s>[INST]{human text} [/INST] </s>

```
- **Human text**: It denotes human instructions to the model. The human text is enclosed within an instruction tag `[INST] [/INST]`
- **Assistant**: represents the context that will assist the model in drawing out a response to the instruction issued by a human. The assistant text is nested to the human text within a segment tag `<s>  </s>`.

One or more segments can exist within a training sample text. A segment can consist of both human instruction text and assistant context or human instruction text only.


<img src="images/template.png"/>

Next, we load the filtered dataset by running the cell below.
"""

dataset = load_dataset('/content/data/filtered')

"""# ë°ì´í„° í¬ë§· ë³€í™˜
llama2 í¬ë§·ì— ë§ì¶°ì„œ
- í”„ë¡¬í”„íŠ¸ê°€  BOS <tag>ì¸ <s>ë¡œ  ì‹œì‘í•´ì•¼ í•˜ê³  EOS <tag>ì¸ </s>ë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤.  
- `###`ê°€ ì•„ë‹Œ `[INST]`ì™€  `[/INST]` <tag>ë¡œ ê°ì‹¸ì•¼ í•©ë‹ˆë‹¤.

Execute the function to transform the filtered dataset to the format explained above.
"""

### credit: https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k ###

# Define a function to transform the data
def transform_to_template(example):
    conversation_text = example['text']
    segments = conversation_text.split('###')

    reformatted_segments = []

    # Iterate over pairs of segments
    for i in range(1, len(segments) - 1, 2):
        human_text = segments[i].strip().replace('Human:', '').strip()

        # Check if there is a corresponding assistant segment before processing
        if i + 1 < len(segments):
            assistant_text = segments[i+1].strip().replace('Assistant:', '').strip()

            # Apply the new template
            reformatted_segments.append(f'<s>[INST] {human_text} [/INST] {assistant_text} </s>')
        else:
            # Handle the case where there is no corresponding assistant segment
            reformatted_segments.append(f'<s>[INST] {human_text} [/INST] </s>')

    return {'text': ''.join(reformatted_segments)}


# Apply the transformation
template_dataset = dataset.map(transform_to_template)

print( len(template_dataset['train']))
print( len(template_dataset['test']))

"""Let's display a sample to inspect if our training set is in the right format as shown in the screenshot above."""

template_dataset['train'][2:3]

"""Save the preprocessed dataset to the directory `../data/ds_preprocess`."""

os.makedirs("/content/data/ds_preprocess", exist_ok=True)
template_dataset.save_to_disk('/content/data/ds_preprocess')

"""## Fine-tuning LLAMA 2

As mentioned earlier in the notebook, our choice model for finetuning is the `Llama-2-7b-chat`. Below is the list of walkthrough steps to complete the task.

- Convert the `Llama-2-7b-chat` to Hugging Face transformer format
- Set the paths to the model and load the dataset
- Load the model tokenizer
- Set the training parameter
- Configure the Parameter Efficient Fine Tuning with LoRA
- Apply 4-bits quantization
- Setup the trainer to start the finetuning process

## **ì£¼ì˜**
ë©”ëª¨ë¦¬ ì´ìŠˆë¡œ ì¸í•´ ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘ì„ í•˜ëŠ” ê²½ìš°  **`ì„¸ì…˜ ë‹¤ì‹œì‹œì‘`**ì„ ëˆ„ë¥´ì‹œë©´ ë©ë‹ˆë‹¤.

ì ˆëŒ€ë¡œ **`ëŸ°íƒ€ì„ ì—°ê²°í•´ì œ`**ë¥¼ ëˆ„ë¥´ë©´ ì•ˆë©ë‹ˆë‹¤.
ì„¤ì¹˜ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë‹¤ìš´ë¡œë“œí•œ ì²´í¬í¬ì¸íŠ¸, ë°ì´í„°ê°€ ëª¨ë‘ ì‚¬ë¼ì§‘ë‹ˆë‹¤.
"""

# In some cases where you have access to limited computing resources, you might have to uncomment os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64" if you run into not enough memory issue


import os
import torch
import json
from datasets import load_dataset, load_from_disk
from langdetect import detect
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
import re
from peft import LoraConfig, PeftModel
from trl import SFTTrainer

import warnings
warnings.filterwarnings("ignore")

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"

"""Now, we can initialize the path to our transformer format model and load the transformed/preprocessed training dataset from the directory where it was saved."""

# initailize path to the base model
base_model = "/content/model/Llama-2-7b-chat-hf"

# set the path to the dataset template
data_path = "/content/data/ds_preprocess/train"

# set the path to the dataset template
eval_path = "/content/data/ds_preprocess/test"

# load the transformed dataset
train_ds = load_from_disk(data_path)
eval_ds = load_from_disk(eval_path)

"""### Loading tokenizer

Tokenization is breaking a text into sentences, words, or sub-words. Each word or sentence in a text is considered as a token. Tokenization allows a detailed text data analysis when broken into smaller units. The [LLaMA tokenizer](https://huggingface.co/docs/transformers/en/model_doc/llama2) is a byte-pair-encoding (BPE) model based on [sentencepiece](https://aclanthology.org/P16-1162/), an unsupervised text tokenizer and detokenizer for Neural Network-based text generation systems that predetermined the vocabulary size prior to the neural model training.

In the cell below, we load the tokenizer from our base model directory and set the parameters:

- **pad_token**: a special token used to make arrays of tokens the same size for batching purposes.
- **padding_side**: side to pad

A comprehensive list of Llama tokenizer parameters can be found [here](https://huggingface.co/docs/transformers/en/model_doc/llama2)
"""

tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

"""### Set Training Hyperparameters

Training hyperparameters are customized using the `TrainingArguments` class. The class provides an API that offers a wide range of options to customize and optimize the training process. Please find a comprehensive description of the hyperparameters [here](https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html#transformers.TrainingArguments). You can further modify the values of the hyperparameters in the next cell after the complete finetune process and rerun the cells to see how it impact on the training outcome.

### Configure PEFT With LoRA

Finetuning large language pre-trained models is computationally costly. Our main goal is to accelerate our finetuning process with minimal memory consumption. A method to achieve that is to use a state-of-the-art [Parameter-Efficient Finetuning (PEFT)](https://github.com/huggingface/peft/tree/main) approach. [PEFT](https://arxiv.org/abs/2305.16742) allows finetuning a small number of (extra) model parameters instead of all the model's parameters, and this significantly decreases the computational and storage costs. One of the ways to implement PEFT is to adopt the Low-Rank Adaptation (LoRA) technique. Lora makes finetuning more efficient by greatly reducing the number of trainable parameters for downstream tasks. It does this by freezing the pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture. According to the [authors of LoRA](https://arxiv.org/abs/2106.09685), Aside from reducing the number of trainable parameters by 10k times, it also reduces the GPU consumption by 3x, thus delivering high throughput with no inference latency. For quick background on LoRA, please follow this [link](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora).

<center><img src="https://raw.githubusercontent.com/openhackathons-org/End-to-End-LLM/refs/heads/main/workspace-llm-use-case/jupyter_notebook/llm-use-case/images/lora-arch.png" height="500px" width="900px"  /></center>
<center>  LoRA reparametrization and Weight merging. <a href="https://huggingface.co/docs/peft/main/en/conceptual_guides/lora"> View source</a> </center>

LoRA techniques are applied through `LoraConfig`, which provides PEFT parameters that control how the method is applied to the base model. A description of the parameter used in the cell below is given as follows:

- **lora_alpha**: LoRA scaling factor
- **lora_dropout**: The dropout probability for LoRA layers.
- **r**: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.
- **bias**: Specifies if the bias parameters should be trained. It can be 'none', 'all', or 'lora_only'.
- **task_type**: Possible task types which include `CAUSAL_LM`, `FEATURE_EXTRACTION`, `QUESTION_ANS`, `SEQ_2_SEQ_LM`, and `SEQ_CLS and TOKEN_CLS`.   

Because the task we want to perform is text generation, we have set the task_type to Causal language model `(CAUSAL_LM)`, which is frequently used for text generation tasks. Please run the cell below to set up the LoRA configuration.
"""

lora_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)

"""### 4-bit quantization configuration

Model quantization is a popular deep-learning optimization method in which model dataâ€”network parameters and activationsâ€”are converted from floating-point to lower-precision representation, typically using 8-bit integers. Quantization represents data with fewer bits, making it a useful technique for reducing memory usage and accelerating inference, especially in large language models (LLMs). It can be combined with PEFT methods to make it easier to train and load LLMs for inference.

<center><img src="https://raw.githubusercontent.com/openhackathons-org/End-to-End-LLM/refs/heads/main/workspace-llm-use-case/jupyter_notebook/llm-use-case/images/quantization.png" height="400px" width="700px" /></center>
<center> <a href="https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/" > source: Using Quantization Aware Training with NVIDIA TensorRT</a></center>

Several ways and algorithms to quantize a model including can be found [here](https://huggingface.co/docs/peft/main/en/developer_guides/quantization). A library to easily implement quantization and integrate with transformers is the `bitsandbytes` library. The library provides config parameters to quantize a model to 8 or 4 bits using the `BitsAndBytesConfig` class. The 4 bits parameters used in the cell below are described as follows:

- **load_in_4bit**: set `True` to quantize the model to 4-bits when you load it
- **bnb_4bit_quant_type**: set to `"nf4"` to use a special 4-bit data type for weights initialized from a normal distribution
- **bnb_4bit_use_double_quant**: set `True` to use a nested quantization scheme to quantize the already quantized weights
- **bnb_4bit_compute_dtype**: set to `torch.float16` or `torch.bfloat16` to use bfloat16 for faster computation

Run the cell below to set the 4-bit quantization for our model.
"""

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False,
)

"""### Loading Base Model

The next step is to load our base model `(Llama-2-7b-chat-hf)` with the causal language model class used for the text generation task. We do this by passing the base model, quantization config, and GPU device ID to the `AutoModelForCausalLM` object, as shown in the cell below
"""

model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=quant_config,
    device_map={"": 0}
)
model.config.use_cache = False
model.config.pretraining_tp = 1

!nvidia-smi

"""### Set the Trainer Hyperparameters

To initiate our model trainer, we create a trainer object from [Supervised fine-tuning (SFT)](https://huggingface.co/docs/trl/en/sft_trainer). SFT is part of the integrated transformer [Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/en/index) tools used to train transformer language models using Reinforcement Learning. Others include [Reward Modeling step (RM)](https://huggingface.co/docs/trl/en/reward_trainer) and  Proximal [Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347). In our SFT trainer object, we set our model, training dataset, PEFT config  object, model tokenizer, and training argument parameter. We also specify the field (`text`) to use within our dataset.

**Note:** *If running on a single DGX A100 GPU, modify the value of `max_seq_length` to 1024 or set it to none (as default).*
"""

from trl import SFTConfig, SFTTrainer


sft_cfg = SFTConfig(
    # â€” data & formatting
    dataset_text_field="text",   # which column to read
    max_seq_length=128,          # truncate / pad length for small Colab gpu memory

    # â€” training schedule
    num_train_epochs=1,
    per_device_train_batch_size=8, # max num for Colab T4 GPU
    gradient_accumulation_steps=1,

    # â€” optimization
    optim="paged_adamw_32bit",
    learning_rate=2e-4,
    weight_decay=0.001,
    lr_scheduler_type="constant",
    warmup_ratio=0.03,
    max_grad_norm=0.3,

    # â€” precision & logging
    bf16=False,
    fp16=False,
    save_steps=25,
    logging_steps=1,

    # â€” output & reporting
    output_dir="/content/model/results",
    save_total_limit=3,
    run_name=None,               # no default W&B run name
    report_to=["tensorboard"],   # ONLY TensorBoard, no "wandb", default turn on wandb

    # â€” packing
    packing=False,               # exampleâ€packing off
    eval_packing=False,
)

trainer = SFTTrainer(
    model,                   #  4-bit + LoRA model
    train_dataset=train_ds,  #  HF train dataset
    args=sft_cfg,            # 3 SFTConfig object
    eval_dataset=eval_ds,    #   eval dataset
    peft_config=lora_config,   #   LoRA settings
    processing_class=tokenizer,  # pass tokenizer
)

trainer.train()

"""Run the cell below to train the SFT trainer object

Save the finetuning model and tokenizer in the directory  `/content/model/Llama-2-7b-chat-hf-finetune`
"""

# save model
new_model = "/content/model/Llama-2-7b-chat-hf-finetune"
trainer.model.save_pretrained(new_model)
trainer.tokenizer.save_pretrained(new_model)

"""## Inferencing

Now that we completed our finetuning process, we can run a quick inference to test how our new model is performing. To do that, we will create the following:

- a transformer pipeline with four parameter inputs: `model`, `tokenizer`, `max_length`, and `task`.  
- format prompt as: `f"<s>[INST] {prompt} [/INST]"`
- pass the prompt into the pipeline object and get result.

```python

inf_pipeline = pipeline(model=model, tokenizer=tokenizer, max_length=200, task="text-generation")
prompt = inf_pipeline(f"<s>[INST] {prompt} [/INST]")
result = inf_pipeline(prompt)
print(result[0]['generated_text'])
```
You can modify the `max_length` to decide the length of text generated by the model.

"""

def run_inference(prompt):
    inf_pipeline = pipeline(model=model, tokenizer=tokenizer, max_length=128, task="text-generation")
    prompt = f"<s>[INST] {prompt} [/INST]"
    result = inf_pipeline(prompt)
    print(result[0]['generated_text'])

prompt = "explain what is astrophotography?"
run_inference(prompt)

"""**Likely output:**

```bash
<s>[INST] explain what is astrophotography? [/INST] Astrophotography is the branch of photography that deals with the photographing of celestial objects such as stars, planets, galaxies, and other astronomical phenomena. Astrophotographers use specialized cameras and telescopes to capture images of these objects, often in low light conditions. Astrophotography requires a great deal of patience, skill, and knowledge, as photographers must be able to accurately track the movement of celestial objects and compensate for the effects of light pollution and atmospheric distortion. Astrophotography has become increasingly popular in recent years, with many amateur astronomers and professional photographers engaging in this hobby and art form.

Astrophotography can be done using a variety of techniques, including long exposure times, tracking mounts, and using narrow-band filters to
```
"""

prompt = "can you explain further?"
run_inference(prompt)

"""**Likely output:**

```bash
<s>[INST] can you explain further? [/INST] Sure, I'd be happy to explain further! Could you please provide more context or clarify what you're asking about? ğŸ˜ƒ ğŸ‘ ğŸ’¡ ğŸ–Š ğŸ“ ğŸ“… ğŸ•° ğŸ•°ï¸ ğŸ•³ï¸ ğŸ•·ï¸ ğŸ•¸ï¸ ğŸ•·ï¸ ğŸ•¸ï¸ ğŸ•³ï¸ ğŸ•·ï¸ ğŸ•¸ï¸ ğŸ•³ï¸ ğŸ•·ï¸ ğŸ•¸ï¸ ğŸ•°ï¸ ğŸ•°ï¸ ğŸ•³ï¸ ğŸ•·ï¸ ğŸ•¸ï¸ ğŸ•³ï¸ ğŸ•·ï¸ ğŸ•¸ï¸ ğŸ•°ï¸

```
"""

prompt = "I want you to explain further on astrophotography"
run_inference(prompt)

"""**Likely output:**

```bash

<s>[INST] I want you to explain futher on astrophotography [/INST] Sure, I d be happy to explain further on astrophotography. Astrophotography is the process of capturing images of celestial objects such as stars, planets, galaxies, and other astronomical phenomena. It involves using specialized cameras and techniques to capture high-quality images of these objects in the night sky.

There are several key components to astrophotography:

    Camera: Astrophotography cameras are designed specifically for capturing images of celestial objects. They typically have large sensors, fast lenses, and specialized features such as built-in equatorial mounts, motorized tracking systems, and cooling systems to reduce noise and improve image quality.

    Telescope: A telescope is used to gather light from the celestial object being photographed. There are several types of

```

### Reload model in FP16 and merge with LoRA weights

To have our model as a single entity for ease of use and widened task coverage, we reload it in fp16 mode and merge it with the LoRA weights using `model.merge_and_unload()`. The tokenizer is reloaded, pad, and saved along with the merged model in the same directory, `/content/model/Llama-2-7b-chat-hf-merged`.

# ë©”ëª¨ë¦¬ ë¶€ì¡±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ `ì„¸ì…˜ ì¬ì‹œì‘`ì´ í•„ìš”í•©ë‹ˆë‹¤.
"""

# In some cases where you have access to limited computing resources, you might have to uncomment os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64" if you run into not enough memory issue


import os
import torch
import json
from datasets import load_dataset, load_from_disk
from langdetect import detect
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
import re
from peft import LoraConfig, PeftModel
from trl import SFTTrainer

import warnings
warnings.filterwarnings("ignore")

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"

# initailize path to the base model
base_model = "/content/model/Llama-2-7b-chat-hf"
new_model = "/content/model/Llama-2-7b-chat-hf-finetune"

# Reload model in FP16 and merge it with LoRA weights
load_base_model = AutoModelForCausalLM.from_pretrained( base_model, torch_dtype=torch.float16, low_cpu_mem_usage=True, return_dict=True, device_map={"": 0})

model = PeftModel.from_pretrained(load_base_model, new_model)
model = model.merge_and_unload()


# Reload tokenizer to save it
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.add_special_tokens({'pad_token': '[PAD]'})
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

model.save_pretrained("/content/model/Llama-2-7b-chat-hf-merged", safe_serialization=True)
tokenizer.save_pretrained("/content/model/Llama-2-7b-chat-hf-merged")

"""**expected output**:
```python
('/content/model/Llama-2-7b-chat-hf-merged/tokenizer_config.json',
 '/content/model/Llama-2-7b-chat-hf-merged/special_tokens_map.json',
 '/content/model/Llama-2-7b-chat-hf-merged/tokenizer.model',
 '/content/model/Llama-2-7b-chat-hf-merged/added_tokens.json',
 '/content/model/Llama-2-7b-chat-hf-merged/tokenizer.json')
```

í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ PEFT finetuneëœ lora adapterë¥¼ baseëª¨ë¸ì— mergeí•˜ì—¬ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì €ì¥í•˜ì˜€ìŠµë‹ˆë‹¤.

# additional work for meerkat model

<img src="https://cdn-uploads.huggingface.co/production/uploads/5efbdc4ac3896117eab961a9/IH0nR9HxYwNvrJBjP2dYQ.png" width=200>

meerkat is medical domain sllm model from mistal-8b from Korea University.

<img src="https://cdn.gttkorea.com/news/photo/202404/9856_10974_3446.png">

[Meerkat-7B-v1.0](https://huggingface.co/dmis-lab/meerkat-7b-v1.0)is an instruction-tuned medical AI system that surpasses the passing threshold of 60% for the United States Medical Licensing Examination (USMLE) for the first time among all 7B-parameter models. The model was trained using [new synthetic dataset](https://huggingface.co/datasets/dmis-lab/meerkat-instructions) consisting of high-quality chain-of-thought reasoning paths sourced from 18 medical textbooks, along with diverse instruction-following datasets. This equips the model with high-level medical reasoning capabilities required for solving complex medical problems. [Nature Paper](https://www.nature.com/articles/s41746-025-01653-8)  [meerkat model](https://huggingface.co/dmis-lab/meerkat-7b-v1.0)

# Instruction-Tuning Llama-2-7b-chat on Meerkat dataset

In this notebook we will:

1. Reset the kernel and Import required libraries (and optionally fix CUDA OOM issues).  
2. Load & merge the nine Meerkat JSONL files into one `Dataset`.  
3. Transform each example into the `<s>[INST]â€¦[/INST]â€¦</s>` template.  
4. Split into train/eval.  
5. Initialize tokenizer, 4-bit quantized model, and LoRA PEFT config.  
6. Set up `SFTTrainer` and `TrainingArguments`.  
7. Train and save as **Llama-2-7b-chat-meerkat**.

Reset the kernel and check the memory
"""

!nvidia-smi

# 1) Imports + optional CUDA memory workaround
import os
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    logging,
)
from peft import LoraConfig
from trl import SFTTrainer

# If you hit CUDA OOM, uncomment this line:
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"

import warnings
warnings.filterwarnings("ignore")                    # ignore Python warnings

"""## 2a) download meerkat dataset
from hugginface

Colabí™˜ê²½ì—ì„œ ì‘ë™ì‹œí‚¤ê¸° ìœ„í•´ì„œ  download util íŒŒì¼ download-meerkat-ds.pyì„ ì—´ì–´ ì•„ë˜ì™€ ê°™ì´ ì €ì¥ ê²½ë¡œë¥¼ ë³€ê²½í•´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.

ì›ë³¸ íŒŒì¼ì€ ì•„ë˜ ì‹¤ìŠµ ìë£Œì‹¤ì— ìˆìŠµë‹ˆë‹¤.
[github](https://github.com/yhgon/bootcamp_kr/raw/refs/heads/main/2025/0703_AIFrenz/)

ìˆ˜ì •ì „
```
    output_dir = os.path.abspath(os.path.join(script_dir, "../../data_meerkat"))
```
ìˆ˜ì • í›„
```
    output_dir = os.path.abspath(os.path.join(script_dir, "/content/data/data_meerkat"))
```
"""



# Commented out IPython magic to ensure Python compatibility.
# %time !python3 /content/source/download-meerkat-ds.py

"""## 2b) Load & merge the Meerkat JSONL files

We point at `../../data_meerkat`, find all `.jsonl` files, and load them into a single `DatasetDict` under the `"train"` split.
"""

# adjust this path if needed
meerkat_dir = "/content/data/data_meerkat"

# collect all JSONL paths
files = [
    os.path.join(meerkat_dir, fname)
    for fname in os.listdir(meerkat_dir)
    if fname.endswith(".jsonl")
]

# load into one big 'train' split
raw = load_dataset("json", data_files={"train": files})
raw_ds = raw["train"]

print(f"Loaded {len(raw_ds):,} total examples.")

"""## 3) Transform examples into the `<s>[INST]â€¦[/INST]â€¦</s>` format

We handle the various Meerkat schemas (`instruction`/`output`, `question`/`answer` + optional CoT `explanation` + `options`) and collapse each into a single string field `"text"`.

"""

def transform_meerkat(example):
    # 3a) if instruction/output format
    if "instruction" in example and "output" in example:
        prompt = example["instruction"].strip()
        response = example["output"].strip()
    # 3b) if QA + CoT format
    elif "question" in example and "answer" in example:
        explan = example.get("explanation", "").strip()
        # final answer sentence
        final = f"{explan} Therefore, the answer is {example['answer']}."
        # prepend any multipleâ€choice options
        opts = example.get("options")
        if isinstance(opts, dict):
            optstr = " ".join(f"({k}) {v}" for k, v in opts.items())
            prompt = example["question"].strip() + " Options: " + optstr
        else:
            prompt = example["question"].strip()
        response = final
    else:
        # fallback to text + answer
        prompt = example.get("text", "").strip()
        response = example.get("answer", "").strip()

    return {"text": f"<s>[INST] {prompt} [/INST] {response} </s>"}

# apply and drop old columns
ds = raw_ds.map(transform_meerkat, remove_columns=raw_ds.column_names)
print("Transformed to template, example:")
print(ds[0]["text"])

"""## 4) Train/Validation Split

We hold out 5% of the data for evaluation.

"""

split = ds.train_test_split(test_size=0.05, seed=42)
train_dataset = split["train"]
eval_dataset  = split["test"]

print(f"â†’ train size: {len(train_dataset):,}")
print(f"â†’ eval  size: {len(eval_dataset):,}")

!nvidia-smi

# 1) Imports + optional CUDA memory workaround
import os
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    logging,
)
from peft import LoraConfig
from trl import SFTTrainer

# If you hit CUDA OOM, uncomment this line:
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"

import warnings
warnings.filterwarnings("ignore")                    # ignore Python warnings

!nvidia-smi

"""## 5) Initialize Tokenizer & Model + LoRA+4-bit Quant

- **Tokenizer** from `Llama-2-7b-chat-hf`  
- **4-bit quant** (NF4) for the model to reduce VRAM.  


"""

base_model = "/content/model/Llama-2-7b-chat-hf"

# tokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# 4-bit quant config
quant_cfg = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False,
)

# load quantized model
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=quant_cfg,
    device_map="auto",
)
model.config.use_cache = False
model.config.pretraining_tp = 1

!nvidia-smi

"""## 6) Set up TrainingArguments & SFTTrainer

- **LoRA PEFT** config r=64, Î±=16, dropout=0.1.
We mirror your previous hyperparameters:  
- 2 epochs, batch_size=1, LR=2e-4, save/log every 25 steps, no fp16/bf16, constant LR schedule, TensorBoard logging.

"""

# LoRA config
lora_config = LoraConfig(
    r=64,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
)

from trl import SFTConfig, SFTTrainer


sft_cfg = SFTConfig(
    # â€” data & formatting
    dataset_text_field="text",   # which column to read
    max_seq_length=128,          # truncate / pad length for small Colab gpu memory

    # â€” training schedule
    num_train_epochs=1,
    per_device_train_batch_size=8, # max num for Colab T4 GPU
    gradient_accumulation_steps=1,

    # â€” optimization
    optim="paged_adamw_32bit",
    learning_rate=2e-4,
    weight_decay=0.001,
    lr_scheduler_type="constant",
    warmup_ratio=0.03,
    max_grad_norm=0.3,

    # â€” precision & logging
    bf16=False,
    fp16=False,
    save_steps=25,
    logging_steps=1,

    # â€” output & reporting
    output_dir="/content/model/results-meerkat",
    save_total_limit=3,
    run_name=None,               # no default W&B run name
    report_to=["tensorboard"],   # ONLY TensorBoard, no "wandb", default turn on wandb

    # â€” packing
    packing=False,               # exampleâ€packing off
    eval_packing=False,
)

"""## 7) Train & Save the Fine-Tuned Model

This will produce **Llama-2-7b-chat-meerkat** under `../../model/`.

"""

trainer = SFTTrainer(
    model,                        #  4-bit + LoRA model
    train_dataset=train_dataset , #  HF train dataset
    args=sft_cfg,                 # 3 SFTConfig object
    eval_dataset=eval_dataset,    #   eval dataset
    peft_config=lora_config,        #   LoRA settings
    processing_class=tokenizer,   # pass tokenizer
)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# trainer.train()

# Commented out IPython magic to ensure Python compatibility.


# save under new name
out_model = "/content/model/Llama-2-7b-chat-meerkat"
# %time trainer.model.save_pretrained(out_model)
trainer.tokenizer.save_pretrained(out_model)

print(f"âœ… Fine-tuned model saved to {out_model}")



"""check papers
- [Med Palm/palm2](https://arxiv.org/abs/2305.09617)
- [Med Gemini](https://research.google/blog/advancing-medical-ai-with-med-gemini/)
- [MedPrompt  ](https://arxiv.org/html/2411.03590v1)

---
## References

- https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
- https://llama.meta.com/llama2
- https://huggingface.co/tasks/text-generation
- https://arxiv.org/abs/2304.07327
- https://huggingface.co/datasets/timdettmers/openassistant-guanaco
- https://huggingface.co/docs/transformers/en/model_doc/llama2
- https://huggingface.co/docs/peft/main/en/developer_guides/quantization

## Licensing

Copyright Â© 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply.
"""