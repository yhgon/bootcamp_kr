{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96ohkCyqZNaM"
   },
   "source": [
    "# SNUH - NVIDIA MONAI BootCamp - Auto3DdSeg Hippocampus\n",
    " <img src=\"https://github.com/Project-MONAI/MONAIBootcamp2021/raw/2f28b64f814a03703667c8ea18cc84f53d6795e4/day1/monai.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1aLtpMbJVNCH",
    "outputId": "40da1e90-8167-45af-e4ca-d783eef3029c"
   },
   "outputs": [],
   "source": [
    "#!pip install -qU \"monai-weekly[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7pcbsphZh8G"
   },
   "source": [
    "### Check GPU Support\n",
    "\n",
    "Running `!nvidia-smi`\n",
    "\n",
    "in a cell will verify this has worked and show you what kind of hardware you have access to.\n",
    "if GPU Memory Usage is no `0 MiB` shutdown all kernels and restart current kernel.\n",
    "- step1. shutdown kernel with following <b>Menu</b> > <b>Kernel</b> > <b>Shut Down All kernels </b>\n",
    "- step2. restart kernelw with following <b>Menu</b> > <b>Kernel</b> > <b>Restart Kernel</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVD7911EVcWI",
    "outputId": "2783c3a4-855a-4b2b-86c0-297b7b31b9ce"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aX4DkxZygKNP"
   },
   "source": [
    "### Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNnKTuzBgKZm",
    "outputId": "8b7c3607-768e-4222-d4c0-82806f5a1264"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "print_config()\n",
    "set_determinism(0)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from monai.apps.auto3dseg import (\n",
    "    DataAnalyzer,\n",
    "    BundleGen,\n",
    "    AlgoEnsembleBestN,\n",
    "    AlgoEnsembleBuilder,\n",
    "    export_bundle_algo_history,\n",
    "    import_bundle_algo_history,\n",
    ")\n",
    "from monai.auto3dseg import algo_to_pickle\n",
    "from monai.bundle.config_parser import ConfigParser\n",
    "\n",
    "from monai.utils.enums import AlgoKeys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up our Dataset and exploring the data\n",
    "#### Setup data directory\n",
    "\n",
    "We'll create a temporary directory for all the MONAI data we're going to be using called temp directory in `~/monai-lab/temp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "\n",
    "root_dir = './autoseg_decathlon'\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpNZjqg3Lq-1"
   },
   "source": [
    "## download dataset \n",
    "\n",
    "it would take 1 minutes to download spleen dataset(1.5GB). You would also use cached dataset\n",
    "\n",
    "You can check Medical Segmentation Decathlon dataset [homepage](http://medicaldecathlon.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "import glob\n",
    "import os\n",
    "\n",
    "#resource = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar\"\n",
    "\n",
    "#msd_task = \"Task01_BrainTumour\"   #   7.09GB \n",
    "#msd_task = \"Task02_Heart\"         # 434.6MB \n",
    "#msd_task = \"Task03_Liver\"         #  26.94GB \n",
    "msd_task = \"Task04_Hippocampus\"   #  27.MB\n",
    "#msd_task = \"Task05_Prostate\"      # 228.7MB \n",
    "#msd_task = \"Task06_Lung\"          #  28.53GB \n",
    "#msd_task = \"Task07_Pancreas\"      #  11.45GB \n",
    "#msd_task = \"Task08_HepaticVessel\" #   8.71GB \n",
    "#msd_task = \"Task09_Spleen\"         #   1.5GB\n",
    "#msd_task = 'Task10_Colon\"         #   5.81MB \n",
    "\n",
    "resource = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/\" + msd_task + \".tar\"\n",
    "\n",
    "compressed_file = os.path.join(root_dir, msd_task + \".tar\")\n",
    "dataroot = os.path.join(root_dir, msd_task)\n",
    "\n",
    "if not os.path.exists(dataroot):\n",
    "    download_and_extract(resource, compressed_file, root_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5-WItNULaj8",
    "outputId": "8c2a4e8f-53f9-42d5-e72c-c80aaac26131"
   },
   "outputs": [],
   "source": [
    "train_images = sorted(\n",
    "    glob.glob(os.path.join(dataroot, \"imagesTr\", \"*.nii.gz\")))\n",
    "train_labels = sorted(\n",
    "    glob.glob(os.path.join(dataroot, \"labelsTr\", \"*.nii.gz\")))\n",
    "data_dicts = [\n",
    "    {\"image\": image_name, \"label\": label_name}\n",
    "    for image_name, label_name in zip(train_images, train_labels)\n",
    "]\n",
    "train_files, val_files = data_dicts[:-9], data_dicts[-9:]\n",
    "\n",
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRQ9g2N2mxfb"
   },
   "source": [
    "### visualize dataset\n",
    "Let's use the nibabel library to visualize and examine the Spleen data in the form of a compressed file `nii.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJs0Jl5pOerR",
    "outputId": "4eef4dc0-9ea8-44a4-a2ad-acac973bfd33"
   },
   "outputs": [],
   "source": [
    "#!pip install SimpleITK   nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAZuobWLL8a9",
    "outputId": "01c47ccc-d23e-4c99-b49d-0a386dd41406"
   },
   "outputs": [],
   "source": [
    "val_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhvAlLBlO8Kb"
   },
   "outputs": [],
   "source": [
    "def nii_loader(filename) :\n",
    "    import nibabel as nib\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    nimg = nib.load( filename )\n",
    "    return nimg.get_fdata() , nimg.affine, nimg.header \n",
    "\n",
    "def visualize( dataset, idx=0, target_layer=10 ):\n",
    "    import matplotlib.pyplot as plt    \n",
    "\n",
    "    image = dataset[idx]['image']\n",
    "    label = dataset[idx]['label']\n",
    "\n",
    "    image_data, image_affine, image_header = nii_loader(image)\n",
    "    label_data, label_affine,   label_header = nii_loader(label)\n",
    "\n",
    "    print(image_data.shape, label_data.shape )\n",
    "    target_image = image_data[:,:,target_layer]\n",
    "    target_label = label_data[:,:,target_layer]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2 ,  figsize=(12,8))\n",
    "\n",
    "    ax1.imshow(target_image, cmap='gray' )\n",
    "    ax1.set_title('image')\n",
    "    ax2.imshow(target_label )\n",
    "    ax2.set_title('GT segmentation')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "GHtMGH2MR4KF",
    "outputId": "6dd5a393-b74e-43dc-a79c-6314742a579a"
   },
   "outputs": [],
   "source": [
    "visualize( val_files, idx=8,  target_layer=14 ) # check different idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset filelist with fold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSD dataset structure follows the following convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(dataroot, \"imagesTs/\")\n",
    "train_dir = os.path.join(dataroot, \"imagesTr/\")\n",
    "label_dir = os.path.join(dataroot, \"labelsTr/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct skeleton JSON to populate with your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist_json = {\"testing\": [], \"training\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Populate JSON with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist_json[\"testing\"] = [\n",
    "    {\"image\": \"./imagesTs/\" + file} for file in os.listdir(test_dir) if (\".nii.gz\" in file) and (\"._\" not in file)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list up testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist_json[\"testing\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Populate with training images and labels in your directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist_json[\"training\"] = [\n",
    "    {\"image\": \"./imagesTr/\" + file, \"label\": \"./labelsTr/\" + file, \"fold\": 0}\n",
    "    for file in os.listdir(train_dir)\n",
    "    if (\".nii.gz\" in file) and (\"._\" not in file)\n",
    "]  # Initialize as single fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list up raining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist_json[\"training\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomise training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(datalist_json[\"training\"])\n",
    "datalist_json[\"training\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split training data into N random folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "fold_size = len(datalist_json[\"training\"]) // num_folds\n",
    "for i in range(num_folds):\n",
    "    for j in range(fold_size):\n",
    "        datalist_json[\"training\"][i * fold_size + j][\"fold\"] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list up final training data with all randomised folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist_json[\"training\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save JSON to file `datalist_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "datalist_file = os.path.join(\"./\", \"msd_\" + msd_task.lower() + \"_folds.json\")\n",
    "with open(datalist_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(datalist_json, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Datalist is saved to {datalist_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can have filelists for train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat $datalist_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a input YAML configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cfg = {\n",
    "    \"name\": msd_task,  # optional, it is only for your own record\n",
    "    \"task\": \"segmentation\",  # optional, it is only for your own record\n",
    "    \"modality\": \"MRI\",  # required\n",
    "    \"datalist\": datalist_file,  # required\n",
    "    \"dataroot\": dataroot,  # required\n",
    "}\n",
    "input = \"./input.yaml\"\n",
    "ConfigParser.export_config_file(input_cfg, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking down the AutoRunner\n",
    "Below is the typical usage of AutoRunner\n",
    "\n",
    "```\n",
    "runner = AutoRunner(input=input)\n",
    "runner.run()\n",
    "```\n",
    "The two lines cover the typical settings in Auto3DSeg and now we are going through the internal APIs calls inside these two lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "When the `analyze` flag is set to `True`, AutoRunner will call `DataAnalyzer` to analyze the datasets and generate a statisical report in YAML. Below is the equivalent Python API calls of `DataAnalyzer`:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = \"./workdir_auto3dseg_hippocampus\"\n",
    "\n",
    "if not os.path.isdir(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "datastats_file = os.path.join(work_dir, \"data_stats.yaml\")\n",
    "analyser = DataAnalyzer(datalist_file, dataroot, output_path=datastats_file)\n",
    "datastat = analyser.get_all_case_stats()\n",
    "\n",
    "print(\"datalist file: \", os.path.abspath(datalist_file))\n",
    "print(\"dataroot path: \", os.path.abspath(dataroot))\n",
    "print(\"datastat path: \", os.path.abspath(datastats_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Besides the Python API call, user can also use command line interface (CLI) provided by the Python Fire:\n",
    "```\n",
    "python -m monai.apps.auto3dseg DataAnalyzer get_all_case_stats \\\n",
    "    --datalist=\"<datalist file>\" \\\n",
    "    --dataroot=\"<dataroot path>\" \\\n",
    "    --output_path=\"<datastat path>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Generation (algo_gen)\n",
    "When the `algo_gen` flag is set to `True`, `AutoRunner` will use `BundleGen` to generate monai bundles from templated algorithms in the working directory.\n",
    "\n",
    "The templated algorithms are customized for the datasets when the `generate` method is called. In detail, the `generate` method will fill the templates using information from the data_stats report. \n",
    "Also, it will copy the necessary scripts (train.py/infer.py) to the algorithm folder. Finally, it will create an algo_object.pkl to save the `Algo` so that it can be instantiated in the local or remote machine. \n",
    "Cross validation is used by default, and `num_fold` can be set to 1 if the users do not want cross validation.\n",
    "\n",
    "Below is the equivalent Python API calls of BundleGen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_generator = BundleGen(\n",
    "    algo_path=work_dir,\n",
    "    data_stats_filename=datastats_file,\n",
    "    data_src_cfg_name=input,\n",
    ")\n",
    "\n",
    "bundle_generator.generate(work_dir, num_fold=5)\n",
    "\n",
    "print(\"algo path: \", os.path.abspath(work_dir))\n",
    "print(\"data_stats file: \", os.path.abspath(datastats_file))\n",
    "print(\"task input file: \", os.path.abspath(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the Python API call, user can also use command line interface (CLI) provided by the user's OS. One example is the following bash commands:\n",
    "\n",
    "```\n",
    "python -m monai.apps.auto3dseg BundleGen generate \\\n",
    "    --algo_path=\"<algo path>\" \\\n",
    "    --data_stats_filename=\"<data_stats file>\" \\\n",
    "    --data_src_cfg_name=\"<task input file>\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and saving the algorithm generation history to the local drive\n",
    "If the users continue to train the algorithms on local system, \n",
    "The history of the algorithm generation can be fetched via `get_history` method of the `BundleGen` object. \n",
    "There also are scenarios that users need to stop the Python process after the `algo_gen`. \n",
    "For example, the users may need to transfer the files to a remote cluster to start the training. \n",
    "`Auto3DSeg` offers a utility function `export_bundle_algo_history` to dump the history to hard drive and recall it by `import_bundle_algo_history`.\n",
    "\n",
    "If the files are copied to a remote system, please ensure the algorithm templates are also copied there. \n",
    "Some functions require the path to instantiate the algorithm class properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = bundle_generator.get_history()\n",
    "export_bundle_algo_history(history)  # save the Algo objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add training parameters to cut down the training time in this notebook (Optional)\n",
    "This step is not required, but for demo purposes, we'll set a limit of the epochs to train the algorithms.\n",
    "\n",
    "Some algorithms in `Auto3DSeg` use epoch to mark the progress of training, while the others use iteration to iterate the loops. \n",
    "Below is the code block to convert num_epoch to iteration style and override all algorithms with the same training parameters for a 1-GPU/2-GPU machine.\n",
    "\n",
    "It is not required for the users to set the `train_param`. The users can use either `train()` or `train({})` if no changes are needed. Then the algorithms will go for the full training and repeat 5 folds.\n",
    "\n",
    "On the other hand, users can also use set `train_param` for each algorithm.\n",
    "\n",
    "For demo purposes, below is a code block to convert num_epoch to iteration style and override all algorithms with the same training parameters. The setup works fine for a machine that has GPUs less than or equal to 8. The datalist in this example is only using a subset of the original dataset. Users need to ensure the number of GPUs is not greater than the number that the training dataset can be partitioned. For example, the following code block is not suitable for a 16-GPU system. In such cases, please change the code block accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 2  # change epoch number to 2 to cut down the notebook running time\n",
    "\n",
    "train_param = {\n",
    "    \"num_epochs_per_validation\": 1,\n",
    "    \"num_images_per_batch\": 2,\n",
    "    \"num_epochs\": max_epochs,\n",
    "    \"num_warmup_epochs\": 1,\n",
    "}\n",
    "\n",
    "print(train_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural networks sequentially\n",
    "The algo_gen history contains Algo object that has multiple methods such as train and predict. We can easily use such APIs to trigger neural network training. By default, `AutoRunnner` will start a training on a single node (single or multiple GPUs) in a seqential manner.\n",
    "\n",
    "`algo_to_pickle` is optional and it will update the dumped Algo objects with the accuracies information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = import_bundle_algo_history(work_dir, only_trained=False)\n",
    "for algo_dict in history:\n",
    "    algo = algo_dict[AlgoKeys.ALGO]\n",
    "    algo.train(train_param)  # can use default params by `algo.train()`\n",
    "    acc = algo.get_score()\n",
    "    algo_to_pickle(algo, template_path=algo.template_path, best_metric=acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "Finally, after the neural networks are trained, AutoRunner will apply the ensemble methods in `Auto3DSeg` to improve the overall performance.\n",
    "\n",
    "Here we used a utility function `import_bundle_algo_history` to load the `Algo` that is trained into the ensemble. With the history loaded, we build an ensemble method and use the method to perform the inference on all testing data.\n",
    "\n",
    "NOTE: Because we need to get the prediction in Python, there is no alternative CLI commands for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from monai.apps.auto3dseg import (\n",
    "    AlgoEnsemble,\n",
    "    AlgoEnsembleBuilder)\n",
    "from monai.utils.enums import AlgoKeys\n",
    "n_best=5\n",
    "history = import_bundle_algo_history(work_dir, only_trained=True)\n",
    "builder = AlgoEnsembleBuilder(history, input)\n",
    "builder.set_ensemble_method(AlgoEnsembleBestN(n_best=n_best))\n",
    "ensembler = builder.get_ensemble()\n",
    "preds = ensembler()\n",
    "print(\"ensemble picked the following best {0:d}:\".format(n_best))\n",
    "for algo in ensembler.get_algo_ensemble():\n",
    "    print(algo[AlgoKeys.ID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_inference(dataroot_dir, work_dir, datalist_file, sim_dim, index):\n",
    "    with open(os.path.join(dataroot_dir, datalist_file), \"r\") as f:\n",
    "        sim_datalist = json.load(f)\n",
    "\n",
    "    dataset = sim_datalist[\"testing\"]\n",
    "\n",
    "    if index < 0 or index >= len(dataset):\n",
    "        print(\"Invalid index.\")\n",
    "        return\n",
    "\n",
    "    entry = dataset[index]\n",
    "    image_name = entry[\"image\"].split(\".\")[0]\n",
    "\n",
    "    prediction_nib = nib.load(os.path.join(work_dir, \"ensemble_output\", image_name + \"_ensemble\" + \".nii.gz\"))\n",
    "    pred = np.array(prediction_nib.dataobj)\n",
    "\n",
    "    img_nib = nib.load(os.path.join(dataroot_dir, entry[\"image\"]))\n",
    "    lbl_nib = nib.load(os.path.join(dataroot_dir, entry[\"label\"]))\n",
    "    img = np.array(img_nib.dataobj)\n",
    "    lbl = np.array(lbl_nib.dataobj)\n",
    "\n",
    "    # Display original image\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img[sim_dim[2] // 2])\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.colorbar(shrink=0.55)\n",
    "\n",
    "    # Display ground truth label\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(lbl[sim_dim[2] // 2])\n",
    "    plt.title(\"Ground Truth Label\")\n",
    "    plt.colorbar(shrink=0.55)\n",
    "\n",
    "    # Display predicted label\n",
    "    plt.subplot(1, 3, 3)\n",
    "    if pred.ndim == 4:\n",
    "        plt.imshow(pred[32, :, :, 1])\n",
    "    else:\n",
    "        plt.imshow(pred[32])\n",
    "    plt.title(\"Predicted Label\")\n",
    "    plt.colorbar(shrink=0.55)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "dataroot_dir = \"./\"\n",
    "work_dir = \"./workdir_auto3dseg_hippocampus\"\n",
    "visualize_inference(dataroot_dir, work_dir, \"./msd_task04_hippocampus_folds.json\", sim_dim=3, index=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other dataset cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instance22 dataset\n",
    "\n",
    "The dataset is from MICCAI 2022 challenge [INSTANCE22: The 2022 Intracranial Hemorrhage Segmentation Challenge on Non-Contrast head CT (NCCT)]((https://instance.grand-challenge.org/)). The solution described here won 2nd place (1st place in terms of Dice score).\n",
    "\n",
    "100 non-contrast head CT volumes of clinically diagnosed patients with different kinds of ICH, (including subdural hemorrhage, epidural hemorrhage, intraventricular hemorrhage, intraparenchymal hemorrhage, and subarachnoid hemorrhage), are used for model training. The size of a CT volume is 512 x 512 x N, where N lies in [20, 70]. The pixel spacing of a CT volume is 0.42mm x 0.42mm x 5mm. The images will be stored in NIFTI files. Voxel-level segmentation annotations are: 0 - Background; 1 - ICH.\n",
    "\n",
    "\n",
    "*`input.yaml`*\n",
    "```\n",
    "name: instance22\n",
    "task: segmentation\n",
    "\n",
    "modality: CT\n",
    "datalist: ./instance22_folds.json # list of files\n",
    "dataroot: /workspace/data/instance22 # data location\n",
    "\n",
    "multigpu: True\n",
    "\n",
    "class_names: [\"val_acc_ich\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`instance22_folds.json`*\n",
    "\n",
    "```\n",
    "{\n",
    "  \"testing\": [\n",
    "    {\"image\": \"evaluation/101.nii.gz\"},\n",
    "    {\"image\": \"evaluation/102.nii.gz\"},\n",
    "    {\"image\": \"evaluation/103.nii.gz\"},\n",
    " ...\n",
    "    {\"image\": \"evaluation/127.nii.gz\"},\n",
    "    {\"image\": \"evaluation/128.nii.gz\"},\n",
    "    {\"image\": \"evaluation/129.nii.gz\"},\n",
    "    {\"image\": \"evaluation/130.nii.gz\"}\n",
    "  ],\n",
    "  \"training\": [\n",
    "    {\n",
    "      \"label\": \"train/label/095.nii.gz\",\n",
    "      \"image\": \"train/data/095.nii.gz\",\n",
    "      \"fold\": 0\n",
    "    },\n",
    "    {\n",
    "      \"label\": \"train/label/066.nii.gz\",\n",
    "      \"image\": \"train/data/066.nii.gz\",\n",
    "      \"fold\": 0\n",
    "    },\n",
    "    {\n",
    "      \"label\": \"train/label/069.nii.gz\",\n",
    "      \"image\": \"train/data/069.nii.gz\",\n",
    "      \"fold\": 0\n",
    "    }, \n",
    "    \n",
    "    ...\n",
    "    \n",
    "    \n",
    "    {\n",
    "      \"label\": \"train/label/052.nii.gz\",\n",
    "      \"image\": \"train/data/052.nii.gz\",\n",
    "      \"fold\": 4\n",
    "    },\n",
    "    {\n",
    "      \"label\": \"train/label/023.nii.gz\",\n",
    "      \"image\": \"train/data/023.nii.gz\",\n",
    "      \"fold\": 4\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hecktor22\n",
    "\n",
    "The HECKTOR22 dataset is from MICCAI 2022 challenge [HEad and NeCK TumOR Segmentation and Outcome Prediction (HECKTOR22)](https://hecktor.grand-challenge.org/). The solution described here won the 1st place in the HECKTOR22 challenge (NVAUTO team):\n",
    "\n",
    "Andriy Myronenko, Md Mahfuzur Rahman Siddiquee, Dong Yang, Yufan He and Daguang Xu: \"Automated head and neck tumor segmentation from 3D PET/CT\". In MICCAI (2022). arXiv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*`input.yaml`*\n",
    "\n",
    "```\n",
    "modality: CT                        # primary modality\n",
    "dataroot: /data/hecktor22           # dataset location\n",
    "datalist: hecktor22_folds.json      # a list of filenames\n",
    "class_names: [tumor, lymph_node]    # names for tensorboard\n",
    "extra_modalities: {image2 : pet}    # a second modality\n",
    "\n",
    "    custom_data_transforms:\n",
    "  - key: after_resample_transforms\n",
    "    path: '.'\n",
    "    transform: {_target_: hecktor_crop_neck_region.HecktorCropNeckRegion, box_size: [200, 200, 310]}\n",
    "\n",
    "image_size_mm_90: [200, 200, 310]\n",
    "resample_resolution: [1, 1, 1]\n",
    "\n",
    "roi_size: [192, 192, 192]       \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*`hecktor22_folds.json`*\n",
    "\n",
    "```\n",
    "{\n",
    "    \"testing\": [\n",
    "        {\n",
    "            \"image\": \"imagesTr/CHUP-049__CT.nii.gz\",\n",
    "            \"image2\": \"imagesTr/CHUP-049__PT.nii.gz\",\n",
    "            \"fold\": 0\n",
    "        },\n",
    "        \n",
    "  ...\n",
    "        {\n",
    "            \"image\": \"imagesTr/HGJ-062__CT.nii.gz\",\n",
    "            \"image2\": \"imagesTr/HGJ-062__PT.nii.gz\",\n",
    "            \"fold\": 0\n",
    "        }\n",
    "    ],\n",
    "    \"training\": [\n",
    "        {\n",
    "            \"image\": \"imagesTr/CHUP-049__CT.nii.gz\",\n",
    "            \"image2\": \"imagesTr/CHUP-049__PT.nii.gz\",\n",
    "            \"label\": \"labelsTr/CHUP-049.nii.gz\",\n",
    "            \"fold\": 0\n",
    "        },\n",
    "        {\n",
    "            \"image\": \"imagesTr/CHUP-034__CT.nii.gz\",\n",
    "            \"image2\": \"imagesTr/CHUP-034__PT.nii.gz\",\n",
    "            \"label\": \"labelsTr/CHUP-034.nii.gz\",\n",
    "            \"fold\": 0\n",
    "        },\n",
    "        \n",
    "   ...\n",
    "        {\n",
    "            \"image\": \"imagesTr/CHUP-015__CT.nii.gz\",\n",
    "            \"image2\": \"imagesTr/CHUP-015__PT.nii.gz\",\n",
    "            \"label\": \"labelsTr/CHUP-015.nii.gz\",\n",
    "            \"fold\": 4\n",
    "        }\n",
    "    ]\n",
    "}        \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  BTCV Dataset\n",
    "For BTCV dataset, under Institutional Review Board (IRB) supervision, 50 abdomen CT scans of were randomly selected from a combination of an ongoing colorectal cancer chemotherapy trial, and a retrospective ventral hernia study. The 50 scans were captured during portal venous contrast phase with variable volume sizes (512 x 512 x 85 - 512 x 512 x 198) and field of views (approx. 280 x 280 x 280 mm3 - 500 x 500 x 650 mm3). The in-plane resolution varies from 0.54 x 0.54 mm2 to 0.98 x 0.98 mm2, while the slice thickness ranges from 2.5 mm to 5.0 mm.\n",
    "\n",
    "- Target: 13 abdominal organs including\n",
    " - Spleen\n",
    " - Right Kidney\n",
    " - Left Kidney\n",
    " - Gallbladder\n",
    " - Esophagus\n",
    " - Liver\n",
    " - Stomach\n",
    " - Aorta\n",
    " - IVC\n",
    " - Portal and Splenic Veins\n",
    " - Pancreas\n",
    " - Right adrenal gland\n",
    " - Left adrenal gland.\n",
    "- Modality: CT\n",
    "-Size: 30 3D volumes (24 Training + 6 Testing)\n",
    "- Challenge: BTCV MICCAI Challenge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`input.yaml`*\n",
    "\n",
    "```\n",
    "name: BTCV\n",
    "task: segmentation\n",
    "\n",
    "modality: CT\n",
    "datalist: ./btcv_folds.json # list of files\n",
    "dataroot: /workspace/data/btcv # data location\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`btcv_fold.json`*\n",
    "\n",
    "```\n",
    "{\n",
    "\"description\": \"btcv yucheng\",\n",
    "\"labels\": {\n",
    "    \"0\": \"background\",\n",
    "    \"1\": \"spleen\",\n",
    "    \"2\": \"rkid\",\n",
    "    \"3\": \"lkid\",\n",
    "    \"4\": \"gall\",\n",
    "    \"5\": \"eso\",\n",
    "    \"6\": \"liver\",\n",
    "    \"7\": \"sto\",\n",
    "    \"8\": \"aorta\",\n",
    "    \"9\": \"IVC\",\n",
    "    \"10\": \"veins\",\n",
    "    \"11\": \"pancreas\",\n",
    "    \"12\": \"rad\",\n",
    "    \"13\": \"lad\"\n",
    "},\n",
    "\"licence\": \"yt\",\n",
    "\"modality\": {\n",
    "    \"0\": \"CT\"\n",
    "},\n",
    "\"name\": \"btcv\",\n",
    "\"numTest\": 20,\n",
    "\"numTraining\": 80,\n",
    "\"reference\": \"Vanderbilt University\",\n",
    "\"release\": \"1.0 06/08/2015\",\n",
    "\"tensorImageSize\": \"3D\",\n",
    "\"test\": [\n",
    "    \"imagesTs/img0061.nii.gz\",\n",
    "    \"imagesTs/img0062.nii.gz\",\n",
    "    ...\n",
    "    \n",
    "    \"imagesTs/img0079.nii.gz\",\n",
    "    \"imagesTs/img0080.nii.gz\"\n",
    "],\n",
    "\"training\": [\n",
    "    {\n",
    "        \"fold\": 4,\n",
    "        \"image\": \"imagesTr/img0001.nii.gz\",\n",
    "        \"label\": \"labelsTr/label0001.nii.gz\"\n",
    "    },\n",
    "    {\n",
    "        \"fold\": 4,\n",
    "        \"image\": \"imagesTr/img0002.nii.gz\",\n",
    "        \"label\": \"labelsTr/label0002.nii.gz\"\n",
    "    },\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    {\n",
    "        \"fold\": 0,\n",
    "        \"image\": \"imagesTr/img0039.nii.gz\",\n",
    "        \"label\": \"labelsTr/label0039.nii.gz\"\n",
    "    },\n",
    "    {\n",
    "        \"fold\": 0,\n",
    "        \"image\": \"imagesTr/img0040.nii.gz\",\n",
    "        \"label\": \"labelsTr/label0040.nii.gz\"\n",
    "    }\n",
    "]\n",
    "}    \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caution !!!\n",
    "### please shutdown all kernels with [Kernel] menu >  [Shutdown All Kernel]  before launch next notebook\n",
    "\n",
    "## Navigation\n",
    "\n",
    "- [01 MONAI transform](./01_getting.ipynb)\n",
    "- [02_end_to_end_pipeline](./02_end_to_end_pipeline.ipynb)\n",
    "- [03_spleen_segment](./03_spleen_segment.ipynb)\n",
    "- [04_Auto3DSeg](./04_Auto3DSeg.ipynb)\n",
    "- [05_Auto3DSeg_hippocampus](./05_Auto3DSeg_hippocampus.ipynb)\n",
    "- [06_digital_pathology_wsi](./06_digital_pathology_wsi.ipynbb)\n",
    "- [07_HoverNet_01_inference](./07_HoverNet_01_inference.ipynb)\n",
    "- [08_HoverNet_02_train](./08_HoverNet_02_train.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/Project-MONAI/MONAIBootcamp2021/raw/2f28b64f814a03703667c8ea18cc84f53d6795e4/day1/monai.png\" width=400>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "moani_bootcamp_2_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
